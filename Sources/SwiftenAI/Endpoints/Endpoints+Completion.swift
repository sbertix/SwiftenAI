//
//  Endpoints+Completion.swift
//  Endpoints
//
//  Created by Stefano Bertagno on 09/01/23.
//

import Foundation

import Requests

public extension SwiftenAI.Endpoints {
    /// A `struct` defining properties
    /// for completion-specific endpoints.
    struct Completion: Encodable {
        /// ID of the model to use.
        /// You can use the List models API to see all of your available models,
        /// or see our Model overview for descriptions of them.
        public var model: String

        /// The prompt(s) to generate completions for, encoded as a string,
        /// array of strings, array of tokens, or array of token arrays.
        ///
        /// Note that <|endoftext|> is the document separator that the
        /// model sees during training, so if a prompt is not specified the model
        /// will generate as if from the beginning of a new document.
        public var prompt: [String]

        /// The suffix that comes after a completion of inserted text.
        public var suffix: String?

        /// The maximum number of tokens used to generate in the completion.
        ///
        /// The token count of your prompt plus `maxTokens` cannot exceed
        /// the model's context length. Most models have a context length of
        /// 2048 tokens (except for the newest models, which support 4096).
        public var maxTokens: Int = 16

        /// What sampling temperature to use. Higher values means the model
        /// will take more risks. Try 0.9 for more creative applications, and 0
        /// (argmax sampling) for ones with a well-defined answer.
        ///
        /// We generally recommend altering this or `topP` but not both.
        public var temperature: Double = 1

        /// An alternative to sampling with temperature, called nucleus sampling,
        /// where the model considers the results of the tokens with `topP`
        /// probability mass. So 0.1 means only the tokens comprising the top
        ///  10% probability mass are considered.
        ///
        /// We generally recommend altering this or temperature but not both.
        public var topP: Double = 1

        /// How many completions to generate for each prompt.
        ///
        /// Because this parameter generates many completions,
        /// it can quickly consume your token quota. Use carefully
        /// and ensure that you have reasonable settings for
        /// `maxTokens` and `stop`.
        public var n: Int = 1

        /// Include the log probabilities on the logprobs most likely
        /// tokens, as well the chosen tokens. For example, if logprobs is 5,
        /// the API will return a list of the 5 most likely tokens. The API will
        /// always return the logprob of the sampled token, so there may
        /// be up to logprobs+1 elements in the response.
        ///
        /// The maximum value for logprobs is 5.
        public var logprobs: Int?

        /// Echo back the prompt in addition to the completion
        public var echo: Bool = false

        /// Up to 4 sequences where the API will stop generating further
        /// tokens. The returned text will not contain the stop sequence.
        /// Defaults to <|endoftext|>.
        public var stop: [String] = ["\u{0003}"]

        /// Number between -2.0 and 2.0. Positive values penalize new
        /// tokens based on whether they appear in the text so far,
        /// increasing the model's likelihood to talk about new topics.
        public var presencePenalty: Double = 0

        /// Number between -2.0 and 2.0. Positive values penalize new
        /// tokens based on their existing frequency in the text so far,
        /// decreasing the model's likelihood to repeat the same line verbatim.
        public var frequencyPenalty: Double = 0

        /// Generates `bestOf` completions server-side and returns the "best"
        /// (the one with the highest log probability per token).
        ///
        /// When used with `n`, `bestOf` controls the number of candidate
        /// completions and `n` specifies how many to return â€“ `bestOf` must
        /// be greater than `n`.
        ///
        /// Because this parameter generates many completions,
        /// it can quickly consume your token quota. Use carefully
        /// and ensure that you have reasonable settings for
        /// `maxTokens` and `stop`.
        public var bestOf: Int = 1

        /// Modify the likelihood of specified tokens appearing in the completion.
        ///
        /// Accepts a json object that maps tokens (specified by their token ID
        /// in the GPT tokenizer) to an associated bias value from -100 to 100.
        /// You can use this tokenizer tool (which works for both GPT-2 and GPT-3)
        /// to convert text to token IDs. Mathematically, the bias is added to the logits
        /// generated by the model prior to sampling. The exact effect will vary
        /// per model, but values between -1 and 1 should decrease or increase
        /// likelihood of selection; values like -100 or 100 should result in a ban
        /// or exclusive selection of the relevant token.
        ///
        /// As an example, you can pass `["50256": -100]` to prevent the <|endoftext|>
        /// token from being generated.
        public var logitBias: [String: Double]

        /// A unique identifier representing your end-user, which can help OpenAI
        /// to monitor and detect abuse.
        public var user: String?

        /// Init.
        ///
        /// - parameters:
        ///     - model: The model.
        ///     - prompts: The prompts to generate completions for.
        ///     - suffix: An optional suffix. Defaults to `nil`.
        ///     - maxTokens: The maximum number of tokens used. Defaults to `16`.
        ///     - temperature: The sampling temperature. Defaults to `1`.
        ///     - topP: The nucleus sampling top-P value. Defaults to `1`.
        ///     - n: The number of completions to generate. Defaults to `1`.
        ///     - logprobs: An optional logprobs toggle. Defaults to `nil`.
        ///     - echo: Whether the prompt should be echoed back with the completion. Defaults to `false`.
        ///     - stops: A sequence where the API will stop generating further tokens. Defauls to empty.
        ///     - presencePenalty: The presence penalty. Defaults to `0`.
        ///     - frequencyPenalty: The frequence penalty. Defaults to `0`.
        ///     - bestOf: The server-side best-of generation value. Defaults to `1`.
        ///     - logitBias: A token based logit bias. Defaults to empty.
        ///     - user: An optional user identifier. Defaults to `nil`.
        public init(
            model: ShortModel,
            prompts: [String],
            suffix: String? = nil,
            maxTokens: Int = 16,
            temperature: Double = 1,
            topP: Double = 1,
            n: Int = 1,
            logprobs: Int? = nil,
            echo: Bool = false,
            stops: [String],
            presencePenalty: Double = 0,
            frequencyPenalty: Double = 0,
            bestOf: Int = 1,
            logitBias: [String: Double] = [:],
            user: String? = nil
        ) {
            self.model = model.id
            self.prompt = prompts
            self.suffix = suffix
            self.maxTokens = maxTokens
            self.temperature = temperature
            self.topP = topP
            self.n = n
            self.logprobs = logprobs
            self.echo = echo
            self.stop = stops
            self.presencePenalty = presencePenalty
            self.frequencyPenalty = frequencyPenalty
            self.bestOf = bestOf
            self.logitBias = logitBias
            self.user = user
        }

        /// Init.
        ///
        /// - parameters:
        ///     - model: The model.
        ///     - prompt: The prompt to generate completions for.
        ///     - suffix: An optional suffix. Defaults to `nil`.
        ///     - maxTokens: The maximum number of tokens used. Defaults to `16`.
        ///     - temperature: The sampling temperature. Defaults to `1`.
        ///     - topP: The nucleus sampling top-P value. Defaults to `1`.
        ///     - n: The number of completions to generate. Defaults to `1`.
        ///     - logprobs: An optional logprobs toggle. Defaults to `nil`.
        ///     - echo: Whether the prompt should be echoed back with the completion. Defaults to `false`.
        ///     - stop: A sequence where the API will stop generating further tokens. Defauls to empty.
        ///     - presencePenalty: The presence penalty. Defaults to `0`.
        ///     - frequencyPenalty: The frequence penalty. Defaults to `0`.
        ///     - bestOf: The server-side best-of generation value. Defaults to `1`.
        ///     - logitBias: A token based logit bias. Defaults to empty.
        ///     - user: An optional user identifier. Defaults to `nil`.
        public init(
            model: ShortModel,
            prompt: String,
            suffix: String? = nil,
            maxTokens: Int = 16,
            temperature: Double = 1,
            topP: Double = 1,
            n: Int = 1,
            logprobs: Int? = nil,
            echo: Bool = false,
            stops: [String],
            presencePenalty: Double = 0,
            frequencyPenalty: Double = 0,
            bestOf: Int = 1,
            logitBias: [String: Double] = [:],
            user: String? = nil
        ) {
            self.init(
                model: model,
                prompts: [prompt],
                suffix: suffix,
                maxTokens: maxTokens,
                temperature: temperature,
                topP: topP,
                n: n,
                logprobs: logprobs,
                echo: echo,
                stops: stops,
                presencePenalty: presencePenalty,
                frequencyPenalty: frequencyPenalty,
                bestOf: bestOf,
                logitBias: logitBias,
                user: user
            )
        }

        /// Init.
        ///
        /// - parameters:
        ///     - model: The model.
        ///     - prompts: The prompts to generate completions for.
        ///     - suffix: An optional suffix. Defaults to `nil`.
        ///     - maxTokens: The maximum number of tokens used. Defaults to `16`.
        ///     - temperature: The sampling temperature. Defaults to `1`.
        ///     - topP: The nucleus sampling top-P value. Defaults to `1`.
        ///     - n: The number of completions to generate. Defaults to `1`.
        ///     - logprobs: An optional logprobs toggle. Defaults to `nil`.
        ///     - echo: Whether the prompt should be echoed back with the completion. Defaults to `false`.
        ///     - stop: A sequence where the API will stop generating further tokens. Defauls to empty.
        ///     - presencePenalty: The presence penalty. Defaults to `0`.
        ///     - frequencyPenalty: The frequence penalty. Defaults to `0`.
        ///     - bestOf: The server-side best-of generation value. Defaults to `1`.
        ///     - logitBias: A token based logit bias. Defaults to empty.
        ///     - user: An optional user identifier. Defaults to `nil`.
        public init(
            model: ShortModel,
            prompts: [String],
            suffix: String? = nil,
            maxTokens: Int = 16,
            temperature: Double = 1,
            topP: Double = 1,
            n: Int = 1,
            logprobs: Int? = nil,
            echo: Bool = false,
            stop: String = "\u{0003}",
            presencePenalty: Double = 0,
            frequencyPenalty: Double = 0,
            bestOf: Int = 1,
            logitBias: [String: Double] = [:],
            user: String? = nil
        ) {
            self.init(
                model: model,
                prompts: prompts,
                suffix: suffix,
                maxTokens: maxTokens,
                temperature: temperature,
                topP: topP,
                n: n,
                logprobs: logprobs,
                echo: echo,
                stops: [stop],
                presencePenalty: presencePenalty,
                frequencyPenalty: frequencyPenalty,
                bestOf: bestOf,
                logitBias: logitBias,
                user: user
            )
        }

        /// Init.
        ///
        /// - parameters:
        ///     - model: The model.
        ///     - prompt: The prompt to generate completions for.
        ///     - suffix: An optional suffix. Defaults to `nil`.
        ///     - maxTokens: The maximum number of tokens used. Defaults to `16`.
        ///     - temperature: The sampling temperature. Defaults to `1`.
        ///     - topP: The nucleus sampling top-P value. Defaults to `1`.
        ///     - n: The number of completions to generate. Defaults to `1`.
        ///     - logprobs: An optional logprobs toggle. Defaults to `nil`.
        ///     - echo: Whether the prompt should be echoed back with the completion. Defaults to `false`.
        ///     - stop: A sequence where the API will stop generating further tokens. Defauls to empty.
        ///     - presencePenalty: The presence penalty. Defaults to `0`.
        ///     - frequencyPenalty: The frequence penalty. Defaults to `0`.
        ///     - bestOf: The server-side best-of generation value. Defaults to `1`.
        ///     - logitBias: A token based logit bias. Defaults to empty.
        ///     - user: An optional user identifier. Defaults to `nil`.
        public init(
            model: ShortModel,
            prompt: String,
            suffix: String? = nil,
            maxTokens: Int = 16,
            temperature: Double = 1,
            topP: Double = 1,
            n: Int = 1,
            logprobs: Int? = nil,
            echo: Bool = false,
            stop: String = "\u{0003}",
            presencePenalty: Double = 0,
            frequencyPenalty: Double = 0,
            bestOf: Int = 1,
            logitBias: [String: Double] = [:],
            user: String? = nil
        ) {
            self.init(
                model: model,
                prompt: prompt,
                suffix: suffix,
                maxTokens: maxTokens,
                temperature: temperature,
                topP: topP,
                n: n,
                logprobs: logprobs,
                echo: echo,
                stops: [stop],
                presencePenalty: presencePenalty,
                frequencyPenalty: frequencyPenalty,
                bestOf: bestOf,
                logitBias: logitBias,
                user: user
            )
        }
    }
}

public extension Endpoints.Completion {
    /// Request to create a completion.
    ///
    /// - returns: Some locked `SingleEndpoint`.
    func process() -> Providers.Lock<Secret, AnySingleEndpoint<Responses.Completion>> {
        .init { secret in
            Single {
                Method(.post)
                Path("https://api.openai.com/v1/completions")
                Headers(secret.headers)
                Headers("application/json", forKey: "Content-Type")
                Body(self, encoder: .default)
                Response(Responses.Completion.self, decoder: .default)
            }.eraseToAnySingleEndpoint()
        }
    }
}
